{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8b27d7fa",
   "metadata": {},
   "source": [
    "# Task - Fine tuning Llama2 7B instruct model on twitter sentiment classification dataset - custom instruction added to the data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1ca3d3",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "050e2206",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftModel,\n",
    "    prepare_model_for_kbit_training,\n",
    "    get_peft_model,\n",
    ")\n",
    "import os, torch\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e4a21b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/pragyan/Desktop/Notebooks/Assignment3/Notebook-Assignment3'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed646e99",
   "metadata": {},
   "source": [
    "# Setting up the model and path directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f632bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the model \n",
    "base_model = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "fine_tuned_model = \"llama2-finetunedSentimentClassificationOneInstruction\"\n",
    "output_dir_forArgs = \"/home/pragyan/Desktop/Notebooks/Assignment3/FineTunedModels-Assignment3/TunedOnSentimentDataOnly/FromArgs\"\n",
    "output_dir_forSave = \"/home/pragyan/Desktop/Notebooks/Assignment3/FineTunedModels-Assignment3/TunedOnSentimentDataOnly/FromSave\"\n",
    "\n",
    "\n",
    "# loading the dataset\n",
    "dataset_name = \"carblacac/twitter-sentiment-analysis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfc21e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pragyan/.local/lib/python3.10/site-packages/datasets/load.py:1461: FutureWarning: The repository for carblacac/twitter-sentiment-analysis contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/carblacac/twitter-sentiment-analysis\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_dataset = load_dataset(dataset_name, split=\"train[0:7000]\")\n",
    "testing_dataset = load_dataset(dataset_name, split=\"test[-50:]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33e952ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "print(len(training_dataset))\n",
    "print(len(testing_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00612b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '@fa6ami86 so happy that salman won.  btw the 14sec clip is truely a teaser', 'feeling': 0}\n",
      "{'text': '@WULFFBOY going to see The Roots in SF', 'feeling': 1}\n"
     ]
    }
   ],
   "source": [
    "print(training_dataset[0])\n",
    "print(testing_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060cd3cc",
   "metadata": {},
   "source": [
    "# Creating the instruction dataset for the fine tuning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "419d440b",
   "metadata": {},
   "outputs": [],
   "source": [
    "instructionPrompt = \"Your task is to classify the the text into one of two feelings. Each feeling has two possible values: 0 indicates the text has a negative sentiment, while 1 indicates a positive feeling.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6b521bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store all the instruct here \n",
    "training_instances = []\n",
    "\n",
    "# loop to create the instruction training data for fine tuning \n",
    "for i in training_dataset:\n",
    "    training_input = instructionPrompt + \" ### Text: \" + i['text'] + \" ### Sentiment: \" + str(i['feeling'])\n",
    "    training_instances.append(training_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "175229f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000\n",
      "Your task is to classify the the text into one of two feelings. Each feeling has two possible values: 0 indicates the text has a negative sentiment, while 1 indicates a positive feeling. ### Text: About to go to bed. Sleeping really late tomorrow!  I am so glad the Tigers won tonight!! ### Sentiment: 1\n"
     ]
    }
   ],
   "source": [
    "print(len(training_instances))\n",
    "print(training_instances[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e858297a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df['originalData'] = training_dataset\n",
    "df['instructionInputForFineTuning'] = training_instances\n",
    "makeDataset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d95e90ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>originalData</th>\n",
       "      <th>instructionInputForFineTuning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'text': '@fa6ami86 so happy that salman won. ...</td>\n",
       "      <td>Your task is to classify the the text into one...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'text': '@phantompoptart .......oops.... I gu...</td>\n",
       "      <td>Your task is to classify the the text into one...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'text': '@bradleyjp decidedly undecided. Depe...</td>\n",
       "      <td>Your task is to classify the the text into one...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'text': '@Mountgrace lol i know! its so frust...</td>\n",
       "      <td>Your task is to classify the the text into one...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'text': '@kathystover Didn't go much of any w...</td>\n",
       "      <td>Your task is to classify the the text into one...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6995</th>\n",
       "      <td>{'text': 'Will someone please enlighten me as ...</td>\n",
       "      <td>Your task is to classify the the text into one...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6996</th>\n",
       "      <td>{'text': 'Ã?Ã¡Â»â¢i phÃÂ°ÃÂ¡ng ÃÂ¡n: mua ...</td>\n",
       "      <td>Your task is to classify the the text into one...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6997</th>\n",
       "      <td>{'text': 'Not liking Setanta Sports  Ah well, ...</td>\n",
       "      <td>Your task is to classify the the text into one...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6998</th>\n",
       "      <td>{'text': '@asafrommaui Awesome! Happy Aloha Fr...</td>\n",
       "      <td>Your task is to classify the the text into one...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6999</th>\n",
       "      <td>{'text': 'Feeling better after a good lie down...</td>\n",
       "      <td>Your task is to classify the the text into one...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           originalData  \\\n",
       "0     {'text': '@fa6ami86 so happy that salman won. ...   \n",
       "1     {'text': '@phantompoptart .......oops.... I gu...   \n",
       "2     {'text': '@bradleyjp decidedly undecided. Depe...   \n",
       "3     {'text': '@Mountgrace lol i know! its so frust...   \n",
       "4     {'text': '@kathystover Didn't go much of any w...   \n",
       "...                                                 ...   \n",
       "6995  {'text': 'Will someone please enlighten me as ...   \n",
       "6996  {'text': 'Ã?Ã¡Â»â¢i phÃÂ°ÃÂ¡ng ÃÂ¡n: mua ...   \n",
       "6997  {'text': 'Not liking Setanta Sports  Ah well, ...   \n",
       "6998  {'text': '@asafrommaui Awesome! Happy Aloha Fr...   \n",
       "6999  {'text': 'Feeling better after a good lie down...   \n",
       "\n",
       "                          instructionInputForFineTuning  \n",
       "0     Your task is to classify the the text into one...  \n",
       "1     Your task is to classify the the text into one...  \n",
       "2     Your task is to classify the the text into one...  \n",
       "3     Your task is to classify the the text into one...  \n",
       "4     Your task is to classify the the text into one...  \n",
       "...                                                 ...  \n",
       "6995  Your task is to classify the the text into one...  \n",
       "6996  Your task is to classify the the text into one...  \n",
       "6997  Your task is to classify the the text into one...  \n",
       "6998  Your task is to classify the the text into one...  \n",
       "6999  Your task is to classify the the text into one...  \n",
       "\n",
       "[7000 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd9688d",
   "metadata": {},
   "source": [
    "# Loading the model and setting up the quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a580afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting up bitsandbites configuration\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit= True,\n",
    "    bnb_4bit_quant_type= \"nf4\",\n",
    "    bnb_4bit_compute_dtype= torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant= False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21a6abda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the tokenizer \n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38cc87dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "556b90ddb7cd4594bd150b333d92c016",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pragyan/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/pragyan/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/pragyan/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/pragyan/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# load the model \n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f1b4a5",
   "metadata": {},
   "source": [
    "# Setting up the LORA Configuration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7940d814",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4405092352"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = prepare_model_for_kbit_training(model)\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\n",
    "        'q_proj',\n",
    "        'k_proj',\n",
    "        'v_proj',\n",
    "        'dense',\n",
    "        'fc1',\n",
    "        'fc2',\n",
    "    ]\n",
    ")\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.get_memory_footprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d19ee7e",
   "metadata": {},
   "source": [
    "# Set the training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7700be99",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments = TrainingArguments(\n",
    "    output_dir= output_dir_forArgs,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=3,\n",
    "    gradient_accumulation_steps=1,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=5000,\n",
    "    logging_steps=1000,\n",
    "    logging_strategy=\"steps\",\n",
    "    learning_rate=2e-4,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    group_by_length=True,\n",
    "    disable_tqdm=False,\n",
    "    report_to=\"none\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c33ab932",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72b5f53fede64497b7f7a1210ceb2554",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pragyan/.local/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:317: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n",
      "/home/pragyan/.local/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=makeDataset,\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length= 2048,\n",
    "    dataset_text_field=\"instructionInputForFineTuning\",\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing= False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5ce8f9",
   "metadata": {},
   "source": [
    "# Training Begins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a502458d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pragyan/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2334' max='2334' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2334/2334 24:24, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.172700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.105600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2334, training_loss=1.1362032461288962, metrics={'train_runtime': 1466.6242, 'train_samples_per_second': 4.773, 'train_steps_per_second': 1.591, 'total_flos': 2.0396907182628864e+16, 'train_loss': 1.1362032461288962, 'epoch': 1.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41a74dd",
   "metadata": {},
   "source": [
    "# Saving the fine tuned model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "14b67fe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('llama2-finetunedSentimentClassificationOneInstruction/tokenizer_config.json',\n",
       " 'llama2-finetunedSentimentClassificationOneInstruction/special_tokens_map.json',\n",
       " 'llama2-finetunedSentimentClassificationOneInstruction/tokenizer.json')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model(output_dir_forSave)\n",
    "trainer.save_model(fine_tuned_model)\n",
    "trainer.tokenizer.save_pretrained(fine_tuned_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734316a5",
   "metadata": {},
   "source": [
    "# Testing the new fine tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cd2a09ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test case\n",
    "j = testing_dataset[3]\n",
    "testPropmpt = instructionPrompt + \" ### Text: \" + j['text'] + \" ### Sentiment: \"\n",
    "testPropmptAll = instructionPrompt + \" ### Text: \" + j['text'] + \" ### Sentiment: \" + str(j['feeling'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0ff1e624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your task is to classify the the text into one of two feelings. Each feeling has two possible values: 0 indicates the text has a negative sentiment, while 1 indicates a positive feeling. ### Text: I hate the V plotarc on True Blood  that poor chubby vampire guy getting staked was so sad ### Sentiment: \n",
      "----------\n",
      "Your task is to classify the the text into one of two feelings. Each feeling has two possible values: 0 indicates the text has a negative sentiment, while 1 indicates a positive feeling. ### Text: I hate the V plotarc on True Blood  that poor chubby vampire guy getting staked was so sad ### Sentiment: 0\n"
     ]
    }
   ],
   "source": [
    "print(testPropmpt)\n",
    "print(\"----------\")\n",
    "print(testPropmptAll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ba831209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bc40ef196e04dfb93be22e07b320707",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your task is to classify the the text into one of two feelings. Each feeling has two possible values: 0 indicates the text has a negative sentiment, while 1 indicates a positive feeling. ### Text: I hate the V plotarc on True Blood  that poor chubby vampire guy getting staked was so sad ### Sentiment: 0\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(task=\"text-generation\",\n",
    "                model=base_model,\n",
    "                tokenizer=tokenizer,\n",
    "                max_new_tokens=1)\n",
    "result = pipe(f\"{testPropmpt}\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cd2b4d0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1f672ca12024b1aba9e690214232294",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your task is to classify the the text into one of two feelings. Each feeling has two possible values: 0 indicates the text has a negative sentiment, while 1 indicates a positive feeling. ### Text: I hate the V plotarc on True Blood  that poor chubby vampire guy getting staked was so sad ### Sentiment: 0\n"
     ]
    }
   ],
   "source": [
    "pipe1 = pipeline(task=\"text-generation\",\n",
    "                model=fine_tuned_model,\n",
    "                tokenizer=fine_tuned_model,\n",
    "                max_new_tokens=1)\n",
    "result1 = pipe1(f\"{testPropmpt}\")\n",
    "print(result1[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5d94dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b14c7ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
